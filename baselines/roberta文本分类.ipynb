{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3362a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../autodl-tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e75624-3a8b-4e39-8924-bb71ab2611e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeddata_name = 'poli'\n",
    "# data_name = 'goss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4985cef-9a79-412e-b92f-bdcb9c7844ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = 2\n",
    "# few_shot = 4\n",
    "# few_shot = 8\n",
    "# few_shot = 16\n",
    "# few_shot = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25f36d2b-698b-40e9-aa7e-330941f5fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "# seed = 2\n",
    "# seed = 3\n",
    "# seed = 4\n",
    "# seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ca0a28f-6c29-4875-a93a-ccfa0e853fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration train-a9f345196c811a9a\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/train-a9f345196c811a9a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('marco rubio portrayed fellow gop senator and presidential hopeful ted cruz as all talk and no action on defense in an interview on meet the press.  \"he talks tough on some of these issues,\" rubio said dec. 13. \"for example, he was going to carpet bomb isis. but the only budget he\\'s ever voted for in his time in the senate is a budget that cut defense spending by more than barack obama proposes we cut it.\"  rubio has tried to portray himself as a strong supporter of the military, while cruz has tried to appeal to hawks as well as libertarians , who typically favor less defense spending.  we decided to fact-check rubios claim that cruz voted for a budget that cut defense spending by more than the democratic president.  cruzs vote on rand pauls budget proposal  rubios campaign pointed to cruzs 2013 vote in favor of a budget proposal by u.s. sen. rand paul, r-ky., who is also running for president. cruz was one of just 18 senators, all republican, to vote in favor of pauls amendment. rubio voted against it. the measure failed.  pauls 2013 proposal emerged after the widespread budget cuts, known as the sequester, went into effect. the sequester dramatically reduced non-war defense spending during the next decade.  rubios campaign pointed to pauls statement in his budget proposal about cutting military spending:  \"this budget proposal does not simply reduce military spending, but provides directives to realign the military for the 21st century,\" paul wrote. \"it seeks to reduce the size and scope of the military complex, including its global footprint to one that is more in line with a policy of containment.\"  under pauls proposal , defense appropriations would have gone from $521 billion in 2014 to $634 billion in 2023. the nonpartisan congressional budget office, meanwhile, projected $588 billion in defense appropriations in 2014 to $731 billion in 2023. that means that paul actually increased year-over-year defense spending, though it did not keep pace with estimated projections to sustain current defense levels.  was pauls proposal a cut for defense?  so why did rubio refer to pauls budget as a \"cut\" if defense spending would rise?  \"we take the budget document at its word that it cuts defense spending and seeks to reduce the size and scope of the military,\" rubio senior adviser joe pounder said.  but experts questioned whether rubio can call pauls proposal a \"cut.\"  \"pauls defense budget was above the budget caps set in the budget control act, so in that respect it was an increase (and the presidents budget was an even larger increase),\" said todd harrison, a defense budget expert at the center for strategic and international studies. \"but pauls defense budget was less than what other republicans were proposing and what the president was proposing, so in that sense it was a cut.\"  pauls 2013 proposal for defense was well below obamas request both at the time and now, harrison said.  christopher preble, at the libertarian cato institute, said he would not call pauls budget a \"cut.\"  \"as is typical in washington-speak, a less-than-expected increase is often cast as a cut,\" he said. \"this is misleading.\"  cruz spokesman brian phillips made a similar argument.  \"so it sounds like rubio is engaging in the time-honored washington cartel tactic of budget gimmickry and is suggesting that a reduction in the rate of increase is equal to a cut when in fact the obama and paul budgets spend more on defense every year,\" phillips said. \"the fact is, in supporting the paul budget, cruz did not support a cut in defense spending, but a more responsible rate of increase.\"  benjamin friedman, a defense expert at cato, pointed to cruzs vote in march in favor of a rubio amendment to boost defense spending over two years rather than pauls amendment which would have boosted it with offsets.  \"this was a budget, so it doesnt make rubio wrong, but it undermines his larger point,\" friedman said.  our ruling  rubio said that the only budget cruz \"ever voted for in his time in the senate is a budget that cut defense spending by more than barack obama proposes we cut it.\"  rubio was referring to cruzs vote in favor of pauls budget proposal in 2013. but rubio mischaracterized pauls plan when he called it a \"cut.\" that proposal included an increase in defense spending each year from 2014 going forward a decade, although it did not keep pace with estimated projections in growth.  however, there is a kernel of truth here in that pauls proposal for defense was below obamas request.  we rate this statement mostly false.',\n",
       " 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_type):\n",
    "        \n",
    "        path = '{0}/{1}/{2}shot/seed{3}/{4}'.format(base_path, data_name, few_shot, seed, dataset_type)\n",
    "        \n",
    "        self.dataset = load_dataset(path=path, split=dataset_type)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['clean_text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "# len(dataset), dataset[1]\n",
    "dataset[1][0], dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "token = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# #加载字典和分词工具\n",
    "# token = BertTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f763a4e7-2530-40df-b20d-c50abad59304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('    target ceo brian cornell announced today the retail giant will be discontinuing sales of the holy bible. the company has come under fire recently from religious groups for its new transgender bathroom policy. the king james bible has been available for purchase at target stores ever since the company was founded as goodfellow dry goods back in 1902.  speaking with cnbc, cornell cited recent protests from what he calls religious extremists as the reason to pull the bible from its shelves. target will no longer cater to religious extremists, said cornell. if that means removing the bible from our shelves, then so be it. cornell said the big box chain is sticking to its guns regarding their bathroom policy, and will be phasing out anything having to do with religion. we believe that everyone, every team member, every guest, and every community, deserves to be treated equally, regardless of their religious beliefs.  more  ~~~~~~~~~~~~~~~~~~~~~~~~  this is the video referenced above:  source: http://ncrenegade.com/editorial/target-to-discontinue-sale-of-holy-bible/',\n",
       " 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e59695a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 500]),\n",
       " torch.Size([2, 500]),\n",
       " torch.Size([2, 500]),\n",
       " tensor([0, 1]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True,\n",
    "                                   return_token_type_ids=True,\n",
    "                                   return_attention_mask=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=2,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f620d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d3d02a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bd44a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6916657090187073 0.5\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "    \n",
    "    out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "275dd1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration test-903f5bc504a53c34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/test to /root/.cache/huggingface/datasets/csv/test-903f5bc504a53c34/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60d6ccb3e044a3da87279f374a73111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f36787892c451d9e8254474b907c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/test-903f5bc504a53c34/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
